{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/01/29 12:35:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from operator import add\n",
    "\n",
    "spark_session = SparkSession.builder\\\n",
    "        .master(\"spark://192.168.2.250:7077\") \\\n",
    "        .appName(\"Lecture1_Example0_with_spark\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "        .config(\"spark.dynamicAllocation.shuffleTracking.enabled\",True)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", False)\\\n",
    "        .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"30s\")\\\n",
    "        .config(\"spark.executor.cores\", 2)\\\n",
    "        .config(\"spark.driver.port\",9999)\\\n",
    "        .config(\"spark.blockManager.port\",10005)\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# RDD API\n",
    "spark_context = spark_session.sparkContext\n",
    "\n",
    "spark_context.setLogLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/29 12:35:10 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 221.5 KiB, free 434.2 MiB)\n",
      "24/01/29 12:35:10 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 434.2 MiB)\n",
      "24/01/29 12:35:10 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on host-192-168-2-88-de1:10005 (size: 32.6 KiB, free: 434.4 MiB)\n",
      "24/01/29 12:35:10 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\n",
      "24/01/29 12:35:10 INFO FileInputFormat: Total input files to process : 1\n",
      "24/01/29 12:35:10 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
      "24/01/29 12:35:10 INFO DAGScheduler: Got job 0 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "24/01/29 12:35:10 INFO DAGScheduler: Final stage: ResultStage 0 (runJob at PythonRDD.scala:181)\n",
      "24/01/29 12:35:10 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/01/29 12:35:10 INFO DAGScheduler: Missing parents: List()\n",
      "24/01/29 12:35:10 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[2] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/01/29 12:35:10 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.7 KiB, free 434.1 MiB)\n",
      "24/01/29 12:35:10 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.8 KiB, free 434.1 MiB)\n",
      "24/01/29 12:35:10 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on host-192-168-2-88-de1:10005 (size: 4.8 KiB, free: 434.4 MiB)\n",
      "24/01/29 12:35:10 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1580\n",
      "24/01/29 12:35:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (PythonRDD[2] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/01/29 12:35:10 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "24/01/29 12:35:11 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20240129123509-0011/0 on worker-20240125135918-192.168.2.250-37761 (192.168.2.250:37761) with 4 core(s)\n",
      "24/01/29 12:35:11 INFO StandaloneSchedulerBackend: Granted executor ID app-20240129123509-0011/0 on hostPort 192.168.2.250:37761 with 4 core(s), 1024.0 MiB RAM\n",
      "24/01/29 12:35:11 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 1 for resource profile id: 0)\n",
      "24/01/29 12:35:11 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20240129123509-0011/0 is now RUNNING\n",
      "24/01/29 12:35:14 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.2.250:33846) with ID 0,  ResourceProfileId 0\n",
      "24/01/29 12:35:14 INFO ExecutorMonitor: New executor 0 has registered (new total is 1)\n",
      "24/01/29 12:35:14 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.250:10005 with 366.3 MiB RAM, BlockManagerId(0, 192.168.2.250, 10005, None)\n",
      "24/01/29 12:35:14 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.2.250, executor 0, partition 0, PROCESS_LOCAL, 7671 bytes) \n",
      "24/01/29 12:35:14 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.2.250:10005 (size: 4.8 KiB, free: 366.3 MiB)\n",
      "24/01/29 12:35:15 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.2.250:10005 (size: 32.6 KiB, free: 366.3 MiB)\n",
      "24/01/29 12:35:15 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1535 ms on 192.168.2.250 (executor 0) (1/1)\n",
      "24/01/29 12:35:15 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "24/01/29 12:35:15 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 56261\n",
      "24/01/29 12:35:15 INFO DAGScheduler: ResultStage 0 (runJob at PythonRDD.scala:181) finished in 5.093 s\n",
      "24/01/29 12:35:15 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/01/29 12:35:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "24/01/29 12:35:15 INFO DAGScheduler: Job 0 finished: runJob at PythonRDD.scala:181, took 5.177793 s\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I am happy to join with you today in what will go down in history as the greatest demonstration for freedom in the history of our nation.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read a file from local filesystem oif your driver\n",
    "lines = spark_context.textFile('/home/ubuntu/i_have_a_dream.txt')\n",
    "lines.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/29 12:35:16 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 221.5 KiB, free 433.9 MiB)\n",
      "24/01/29 12:35:16 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 433.9 MiB)\n",
      "24/01/29 12:35:16 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on host-192-168-2-88-de1:10005 (size: 32.6 KiB, free: 434.3 MiB)\n",
      "24/01/29 12:35:16 INFO SparkContext: Created broadcast 2 from textFile at NativeMethodAccessorImpl.java:0\n",
      "24/01/29 12:35:16 INFO BlockManagerInfo: Removed broadcast_1_piece0 on host-192-168-2-88-de1:10005 in memory (size: 4.8 KiB, free: 434.3 MiB)\n",
      "24/01/29 12:35:16 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.2.250:10005 in memory (size: 4.8 KiB, free: 366.3 MiB)\n",
      "24/01/29 12:35:16 INFO FileInputFormat: Total input files to process : 1\n",
      "24/01/29 12:35:16 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
      "24/01/29 12:35:16 INFO DAGScheduler: Got job 1 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "24/01/29 12:35:16 INFO DAGScheduler: Final stage: ResultStage 1 (runJob at PythonRDD.scala:181)\n",
      "24/01/29 12:35:16 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/01/29 12:35:16 INFO DAGScheduler: Missing parents: List()\n",
      "24/01/29 12:35:16 INFO DAGScheduler: Submitting ResultStage 1 (PythonRDD[5] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/01/29 12:35:16 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 7.8 KiB, free 433.9 MiB)\n",
      "24/01/29 12:35:16 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.8 KiB, free 433.9 MiB)\n",
      "24/01/29 12:35:16 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on host-192-168-2-88-de1:10005 (size: 4.8 KiB, free: 434.3 MiB)\n",
      "24/01/29 12:35:16 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1580\n",
      "24/01/29 12:35:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (PythonRDD[5] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/01/29 12:35:16 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
      "24/01/29 12:35:16 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.2.250, executor 0, partition 0, ANY, 7679 bytes) \n",
      "24/01/29 12:35:16 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.2.250:10005 (size: 4.8 KiB, free: 366.3 MiB)\n",
      "24/01/29 12:35:16 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.2.250:10005 (size: 32.6 KiB, free: 366.2 MiB)\n",
      "24/01/29 12:35:17 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 776 ms on 192.168.2.250 (executor 0) (1/1)\n",
      "24/01/29 12:35:17 INFO DAGScheduler: ResultStage 1 (runJob at PythonRDD.scala:181) finished in 0.795 s\n",
      "24/01/29 12:35:17 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/01/29 12:35:17 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "24/01/29 12:35:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "24/01/29 12:35:17 INFO DAGScheduler: Job 1 finished: runJob at PythonRDD.scala:181, took 0.809594 s\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am happy to join with you today in what will go down in history as the greatest demonstration for freedom in the history of our nation.\n",
      "total words= 1680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/29 12:35:17 INFO SparkContext: Starting job: reduce at /tmp/ipykernel_56758/3394129695.py:11\n",
      "24/01/29 12:35:17 INFO DAGScheduler: Got job 2 (reduce at /tmp/ipykernel_56758/3394129695.py:11) with 2 output partitions\n",
      "24/01/29 12:35:17 INFO DAGScheduler: Final stage: ResultStage 2 (reduce at /tmp/ipykernel_56758/3394129695.py:11)\n",
      "24/01/29 12:35:17 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/01/29 12:35:17 INFO DAGScheduler: Missing parents: List()\n",
      "24/01/29 12:35:17 INFO DAGScheduler: Submitting ResultStage 2 (PythonRDD[6] at reduce at /tmp/ipykernel_56758/3394129695.py:11), which has no missing parents\n",
      "24/01/29 12:35:17 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 9.1 KiB, free 433.9 MiB)\n",
      "24/01/29 12:35:17 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 433.9 MiB)\n",
      "24/01/29 12:35:17 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on host-192-168-2-88-de1:10005 (size: 5.5 KiB, free: 434.3 MiB)\n",
      "24/01/29 12:35:17 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1580\n",
      "24/01/29 12:35:17 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 2 (PythonRDD[6] at reduce at /tmp/ipykernel_56758/3394129695.py:11) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/01/29 12:35:17 INFO TaskSchedulerImpl: Adding task set 2.0 with 2 tasks resource profile 0\n",
      "24/01/29 12:35:17 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (192.168.2.250, executor 0, partition 0, ANY, 7679 bytes) \n",
      "24/01/29 12:35:17 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 3) (192.168.2.250, executor 0, partition 1, ANY, 7679 bytes) \n",
      "24/01/29 12:35:17 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.2.250:10005 (size: 5.5 KiB, free: 366.2 MiB)\n",
      "24/01/29 12:35:17 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 3) in 76 ms on 192.168.2.250 (executor 0) (1/2)\n",
      "24/01/29 12:35:17 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 77 ms on 192.168.2.250 (executor 0) (2/2)\n",
      "24/01/29 12:35:17 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "24/01/29 12:35:17 INFO DAGScheduler: ResultStage 2 (reduce at /tmp/ipykernel_56758/3394129695.py:11) finished in 0.096 s\n",
      "24/01/29 12:35:17 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/01/29 12:35:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
      "24/01/29 12:35:17 INFO DAGScheduler: Job 2 finished: reduce at /tmp/ipykernel_56758/3394129695.py:11, took 0.105573 s\n"
     ]
    }
   ],
   "source": [
    "# The same example, this time using map and reduce from the Spark API, and loading the text file from HDFS.\n",
    "\n",
    "lines = spark_context.textFile(\"hdfs://192.168.2.250:9000/i_have_a_dream.txt\")\n",
    "#lines = spark_context.textFile(\"/home/ubuntu/i_have_a_dream.txt\")\n",
    "print(lines.first())\n",
    "\n",
    "words = lines.map(lambda line: line.split(' '))\n",
    "\n",
    "word_counts = words.map(lambda w: len(w))\n",
    "\n",
    "total_words = word_counts.reduce(add)\n",
    "\n",
    "print(f'total words= {total_words}')  \n",
    "\n",
    "# ... the same number of words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/29 12:35:17 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
      "24/01/29 12:35:17 INFO DAGScheduler: Got job 3 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "24/01/29 12:35:17 INFO DAGScheduler: Final stage: ResultStage 3 (runJob at PythonRDD.scala:181)\n",
      "24/01/29 12:35:17 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/01/29 12:35:17 INFO DAGScheduler: Missing parents: List()\n",
      "24/01/29 12:35:17 INFO DAGScheduler: Submitting ResultStage 3 (PythonRDD[7] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/01/29 12:35:17 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 7.8 KiB, free 433.9 MiB)\n",
      "24/01/29 12:35:17 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.8 KiB, free 433.9 MiB)\n",
      "24/01/29 12:35:17 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on host-192-168-2-88-de1:10005 (size: 4.8 KiB, free: 434.3 MiB)\n",
      "24/01/29 12:35:17 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1580\n",
      "24/01/29 12:35:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (PythonRDD[7] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/01/29 12:35:17 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
      "24/01/29 12:35:17 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 4) (192.168.2.250, executor 0, partition 0, ANY, 7679 bytes) \n",
      "24/01/29 12:35:17 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.2.250:10005 in memory (size: 4.8 KiB, free: 366.2 MiB)\n",
      "24/01/29 12:35:17 INFO BlockManagerInfo: Removed broadcast_3_piece0 on host-192-168-2-88-de1:10005 in memory (size: 4.8 KiB, free: 434.3 MiB)\n",
      "24/01/29 12:35:17 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.2.250:10005 (size: 4.8 KiB, free: 366.2 MiB)\n",
      "24/01/29 12:35:17 INFO BlockManagerInfo: Removed broadcast_4_piece0 on host-192-168-2-88-de1:10005 in memory (size: 5.5 KiB, free: 434.3 MiB)\n",
      "24/01/29 12:35:17 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 192.168.2.250:10005 in memory (size: 5.5 KiB, free: 366.2 MiB)\n",
      "24/01/29 12:35:17 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 4) in 48 ms on 192.168.2.250 (executor 0) (1/1)\n",
      "24/01/29 12:35:17 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "24/01/29 12:35:17 INFO DAGScheduler: ResultStage 3 (runJob at PythonRDD.scala:181) finished in 0.076 s\n",
      "24/01/29 12:35:17 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/01/29 12:35:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
      "24/01/29 12:35:17 INFO DAGScheduler: Job 3 finished: runJob at PythonRDD.scala:181, took 0.083900 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['I am happy to join with you today in what will go down in history as the greatest demonstration for freedom in the history of our nation.',\n",
       " '',\n",
       " 'Five score years ago, a great American, in whose symbolic shadow we stand today, signed the Emancipation Proclamation. This momentous decree came as a great beacon light of hope to millions of Negro slaves who had been seared in the flames of withering injustice. It came as a joyous daybreak to end the long night of their captivity.',\n",
       " '',\n",
       " 'But one hundred years later, the Negro still is not free. One hundred years later, the life of the Negro is still sadly crippled by the manacles of segregation and the chains of discrimination. One hundred years later, the Negro lives on a lonely island of poverty in the midst of a vast ocean of material prosperity. One hundred years later, the Negro is still languishing in the corners of American society and finds himself an exile in his own land. So we have come here today to dramatize a shameful condition.',\n",
       " '',\n",
       " \"In a sense we have come to our nation's capital to cash a check. When the architects of our republic wrote the magnificent words of the Constitution and the Declaration of Independence, they were signing a promissory note to which every American was to fall heir. This note was a promise that all men, yes, black men as well as white men, would be guaranteed the unalienable rights of life, liberty, and the pursuit of happiness.\",\n",
       " '',\n",
       " 'It is obvious today that America has defaulted on this promissory note insofar as her citizens of color are concerned. Instead of honoring this sacred obligation, America has given the Negro people a bad check, a check which has come back marked \"insufficient funds.\" But we refuse to believe that the bank of justice is bankrupt. We refuse to believe that there are insufficient funds in the great vaults of opportunity of this nation. So we have come to cash this check -- a check that will give us upon demand the riches of freedom and the security of justice. We have also come to this hallowed spot to remind America of the fierce urgency of now. This is no time to engage in the luxury of cooling off or to take the tranquilizing drug of gradualism. Now is the time to make real the promises of democracy. Now is the time to rise from the dark and desolate valley of segregation to the sunlit path of racial justice. Now is the time to lift our nation from the quick sands of racial injustice to the solid rock of brotherhood. Now is the time to make justice a reality for all of God\\'s children.',\n",
       " '']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/29 12:35:17 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
      "24/01/29 12:35:17 INFO DAGScheduler: Got job 4 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "24/01/29 12:35:17 INFO DAGScheduler: Final stage: ResultStage 4 (runJob at PythonRDD.scala:181)\n",
      "24/01/29 12:35:17 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/01/29 12:35:17 INFO DAGScheduler: Missing parents: List()\n",
      "24/01/29 12:35:17 INFO DAGScheduler: Submitting ResultStage 4 (PythonRDD[8] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/01/29 12:35:17 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 8.4 KiB, free 433.9 MiB)\n",
      "24/01/29 12:35:17 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 5.2 KiB, free 433.9 MiB)\n",
      "24/01/29 12:35:17 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on host-192-168-2-88-de1:10005 (size: 5.2 KiB, free: 434.3 MiB)\n",
      "24/01/29 12:35:17 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1580\n",
      "24/01/29 12:35:17 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 192.168.2.250:10005 in memory (size: 4.8 KiB, free: 366.2 MiB)\n",
      "24/01/29 12:35:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (PythonRDD[8] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/01/29 12:35:17 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0\n",
      "24/01/29 12:35:17 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 5) (192.168.2.250, executor 0, partition 0, ANY, 7679 bytes) \n",
      "24/01/29 12:35:17 INFO BlockManagerInfo: Removed broadcast_5_piece0 on host-192-168-2-88-de1:10005 in memory (size: 4.8 KiB, free: 434.3 MiB)\n",
      "24/01/29 12:35:17 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.2.250:10005 (size: 5.2 KiB, free: 366.2 MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'happy', 'to', 'join', 'with', 'you', 'today', 'in', 'what', 'will', 'go', 'down', 'in', 'history', 'as', 'the', 'greatest', 'demonstration', 'for', 'freedom', 'in', 'the', 'history', 'of', 'our', 'nation.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/29 12:35:17 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 5) in 60 ms on 192.168.2.250 (executor 0) (1/1)\n",
      "24/01/29 12:35:17 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "24/01/29 12:35:17 INFO DAGScheduler: ResultStage 4 (runJob at PythonRDD.scala:181) finished in 0.088 s\n",
      "24/01/29 12:35:17 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/01/29 12:35:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished\n",
      "24/01/29 12:35:17 INFO DAGScheduler: Job 4 finished: runJob at PythonRDD.scala:181, took 0.094565 s\n"
     ]
    }
   ],
   "source": [
    "lines_splitted = lines.map(lambda line: line.split(' '))\n",
    "print(lines_splitted.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/29 12:35:18 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
      "24/01/29 12:35:18 INFO DAGScheduler: Got job 5 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "24/01/29 12:35:18 INFO DAGScheduler: Final stage: ResultStage 5 (runJob at PythonRDD.scala:181)\n",
      "24/01/29 12:35:18 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/01/29 12:35:18 INFO DAGScheduler: Missing parents: List()\n",
      "24/01/29 12:35:18 INFO DAGScheduler: Submitting ResultStage 5 (PythonRDD[9] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/01/29 12:35:18 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 8.5 KiB, free 433.9 MiB)\n",
      "24/01/29 12:35:18 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 5.3 KiB, free 433.9 MiB)\n",
      "24/01/29 12:35:18 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on host-192-168-2-88-de1:10005 (size: 5.3 KiB, free: 434.3 MiB)\n",
      "24/01/29 12:35:18 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1580\n",
      "24/01/29 12:35:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (PythonRDD[9] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/01/29 12:35:18 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0\n",
      "24/01/29 12:35:18 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 192.168.2.250:10005 in memory (size: 5.2 KiB, free: 366.2 MiB)\n",
      "24/01/29 12:35:18 INFO BlockManagerInfo: Removed broadcast_6_piece0 on host-192-168-2-88-de1:10005 in memory (size: 5.2 KiB, free: 434.3 MiB)\n",
      "24/01/29 12:35:18 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 6) (192.168.2.250, executor 0, partition 0, ANY, 7679 bytes) \n",
      "24/01/29 12:35:18 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.2.250:10005 (size: 5.3 KiB, free: 366.2 MiB)\n",
      "24/01/29 12:35:18 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 6) in 57 ms on 192.168.2.250 (executor 0) (1/1)\n",
      "24/01/29 12:35:18 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "24/01/29 12:35:18 INFO DAGScheduler: ResultStage 5 (runJob at PythonRDD.scala:181) finished in 0.089 s\n",
      "24/01/29 12:35:18 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/01/29 12:35:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished\n",
      "24/01/29 12:35:18 INFO DAGScheduler: Job 5 finished: runJob at PythonRDD.scala:181, took 0.098959 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'am',\n",
       " 'happy',\n",
       " 'to',\n",
       " 'join',\n",
       " 'with',\n",
       " 'you',\n",
       " 'today',\n",
       " 'in',\n",
       " 'what',\n",
       " 'will',\n",
       " 'go',\n",
       " 'down',\n",
       " 'in',\n",
       " 'history',\n",
       " 'as',\n",
       " 'the',\n",
       " 'greatest',\n",
       " 'demonstration',\n",
       " 'for']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note, we're in Python, but using Java naming conventions!\n",
    "\n",
    "all_words = lines.flatMap(lambda line: line.split(' '))\n",
    "all_words.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/29 12:35:18 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
      "24/01/29 12:35:18 INFO DAGScheduler: Got job 6 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "24/01/29 12:35:18 INFO DAGScheduler: Final stage: ResultStage 6 (runJob at PythonRDD.scala:181)\n",
      "24/01/29 12:35:18 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/01/29 12:35:18 INFO DAGScheduler: Missing parents: List()\n",
      "24/01/29 12:35:18 INFO DAGScheduler: Submitting ResultStage 6 (PythonRDD[10] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/01/29 12:35:18 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 9.0 KiB, free 433.9 MiB)\n",
      "24/01/29 12:35:18 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 433.9 MiB)\n",
      "24/01/29 12:35:18 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on host-192-168-2-88-de1:10005 (size: 5.5 KiB, free: 434.3 MiB)\n",
      "24/01/29 12:35:18 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1580\n",
      "24/01/29 12:35:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (PythonRDD[10] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/01/29 12:35:18 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n",
      "24/01/29 12:35:18 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 7) (192.168.2.250, executor 0, partition 0, ANY, 7679 bytes) \n",
      "24/01/29 12:35:18 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.2.250:10005 (size: 5.5 KiB, free: 366.2 MiB)\n",
      "24/01/29 12:35:18 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 7) in 61 ms on 192.168.2.250 (executor 0) (1/1)\n",
      "24/01/29 12:35:18 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
      "24/01/29 12:35:18 INFO DAGScheduler: ResultStage 6 (runJob at PythonRDD.scala:181) finished in 0.080 s\n",
      "24/01/29 12:35:18 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/01/29 12:35:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\n",
      "24/01/29 12:35:18 INFO DAGScheduler: Job 6 finished: runJob at PythonRDD.scala:181, took 0.085334 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['down',\n",
       " 'demonstration',\n",
       " 'decree',\n",
       " 'daybreak',\n",
       " 'discrimination.',\n",
       " 'dramatize',\n",
       " 'defaulted',\n",
       " 'demand',\n",
       " 'drug',\n",
       " 'democracy.',\n",
       " 'dark',\n",
       " 'desolate',\n",
       " 'discontent',\n",
       " 'day',\n",
       " 'deeds.',\n",
       " 'drinking',\n",
       " 'dignity',\n",
       " 'discipline.',\n",
       " 'degenerate',\n",
       " 'distrust']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words.filter(lambda word: word.startswith('d'))\\\n",
    "         .take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/29 12:35:18 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
      "24/01/29 12:35:18 INFO SparkUI: Stopped Spark web UI at http://host-192-168-2-88-de1:4040\n",
      "24/01/29 12:35:18 INFO StandaloneSchedulerBackend: Shutting down all executors\n",
      "24/01/29 12:35:18 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down\n",
      "24/01/29 12:35:18 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "24/01/29 12:35:18 INFO MemoryStore: MemoryStore cleared\n",
      "24/01/29 12:35:18 INFO BlockManager: BlockManager stopped\n",
      "24/01/29 12:35:18 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "24/01/29 12:35:18 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "24/01/29 12:35:18 INFO SparkContext: Successfully stopped SparkContext\n"
     ]
    }
   ],
   "source": [
    "# release the cores for another application!\n",
    "spark_context.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
